{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "symptoms_only_train_df = pd.read_csv('data/symptoms-only-train.csv')\n",
    "symptoms_only_test_df = pd.read_csv('data/symptoms-only-test.csv')\n",
    "\n",
    "symptoms_with_conditions_train_df = pd.read_csv('data/symptoms-with-conditions-train.csv')\n",
    "symptoms_with_conditions_test_df = pd.read_csv('data/symptoms-with-conditions-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate symptoms only performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = (\n",
    "    symptoms_only_train_df.drop(\"classificacaoFinal\", axis=1),\n",
    "    symptoms_only_train_df[\"classificacaoFinal\"],\n",
    "    symptoms_only_test_df.drop(\"classificacaoFinal\", axis=1),\n",
    "    symptoms_only_test_df[\"classificacaoFinal\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(best_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_true=y_test, y_pred=y_pred),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Confirmed\", \"Discarded\"],\n",
    "    yticklabels=[\"Confirmed\", \"Discarded\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance metrics radar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "report = classification_report(y_true=y_test, y_pred=y_pred, output_dict=True)\n",
    "roc_auc = roc_auc_score(y_test, y_pred) * 100\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": report[\"accuracy\"] * 100,\n",
    "    \"Precision\": report[\"weighted avg\"][\"precision\"] * 100,\n",
    "    \"Recall\": report[\"weighted avg\"][\"recall\"] * 100,\n",
    "    \"F1 Score\": report[\"weighted avg\"][\"f1-score\"] * 100,\n",
    "    \"ROC AUC\": roc_auc\n",
    "}\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "labels = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "\n",
    "values += values[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "ax.fill(angles, values, color=\"blue\", alpha=0.25)\n",
    "ax.plot(angles, values, color=\"blue\", linewidth=2)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 101, 20))\n",
    "ax.set_yticklabels([f'{i}%' for i in np.arange(0, 101, 20)])\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(\n",
    "    labels,\n",
    ")\n",
    "\n",
    "for label, angle in zip(ax.get_xticklabels(), angles[:-1]):\n",
    "    if label.get_text() in [\"Accuracy\", \"Recall\", \"F1 Score\"]:\n",
    "        label.set_rotation(45)\n",
    "        label.set_rotation_mode('anchor')\n",
    "        label.set_va('center')\n",
    "        label.set_ha('center')\n",
    "        label.set_position((label.get_position()[0], label.get_position()[1] + -0.1))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sars-decision-tree-FdHgHNSR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
